--- 
title: "Analítica Urbana"
author: "Antonio Vazquez Brust"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "Un manual para la obtención, procesamiento, análisis y visualización de datos urbanos"
---

# ¿Para quién es esto? {-}

Este libro fue escrito pensando en aquellas personas que trabajan, investigan y enseñan en áreas relacionadas al hábitat urbano y sus políticas públicas.

Esperamos que el tono introductorio del texto, así como el esfuerzo puesto en explicar los conceptos con la mayor simplicidad posible, resulten de interés para un público amplio. (así decía en CDpGS, lo dejo como antecedente.. lo reescribimos)  


## Antes de empezar {-}

Se requiere conocimiento básico del lenguaje de programación `R`, y del "paquete" de funciones para manipulación y visualización de datos llamado `Tidyverse`. Todo ello puede adquirirse pasando un tiempo con [Ciencia de Datos para Gente Sociable](https://bit.ly/datasoc), que además de gratuito y disponible en línea, es el manual que sirve como base para éste que están leyendo ahora.

Para practicar los ejemplos que se explicarán a lo largo del libro es necesario instalar el [lenguaje de programación R](https://cloud.r-project.org/), y la interfaz gráfica [RStudio Desktop](https://www.rstudio.com/products/rstudio/download/). 

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# Introducción {#intro}

Una bonita introducción

## Sobre la análitica urbana

### ¿Qué es?

### ¿Para qué se usa?

### QUIZAS ALGUN ITEM MAS

<!--chapter:end:01-intro.Rmd-->

# Acceso a información urbana georeferenciada en repositorios _online_

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

# Para que no fallen azarosamente las consultas API 
httr::set_config(httr::config(http_version = 0))
```


[OpenStreetMap](openstreetmap.org) es un servicio de mapas online que publica información contribuida en forma libre por más de un millón de voluntarios, que benefician a los 5,5 millones de usuarios de la plataforma. 

Los contribuidores más entusiastas mapean barrios completos utilizando herramientas GPS para enviar información local completa, actualizada y precisa a OpenStreetMap. Varias empresas y entidades públicas que producen información geográfica también contribuyen al permitir que sus datos sean incluidos. Existen equipos profesionales de contribuidores que que se coordinan para agregar y mantener actualizada información georeferenciada de límites políticos, calles, edificios, negocios y otros puntos de interés; en ocasiones empleados por compañías que dependen de OpenStreetMap para el "mapa base" de sus productos, como mapbox.com y carto.com. 

Toda la información disponible en OpenStreetMap puede ser descargada y reutilizada por cualquier persona, ya sea accediendo al mapa online, obteniendo una copia completa de la base de datos, o accediendo a los datos vía API.

## OpenStreetMap desde R

Utilizaremos [`osmdata`](https://raw.githubusercontent.com/ropensci/osmdata/master/man/figures/title.png), un paquete de R que permite acceder a los datos de OpenStreetMap (OSM de aquí en más) con sus atributos, geometría y posición. 

![](https://raw.githubusercontent.com/ropensci/osmdata/master/man/figures/title.png)

Como siempre, si no tenemos aún el paquete lo instalamos:

```{r eval=FALSE}
library(osmdata)
```

Y lo activamos junto a otros paquetes que vamos a utilizar:

```{r}
library(osmdata)
library(tidyverse) # nuestra navaja suiza para manipulación y visualización de datos
library(sf) # para procesar info espacial
library(leaflet) # Para generar mapas interactivos
```

### Definiendo el lugar 

Antes de descargar información, definimos el lugar que queremos consultar. Éste puede ser un barrio, un municipio, un país, un continente... en éste caso, lo intentaremos con la ciudad de Rosario.

Las funciones de `osmdata` nos permiten realizar consultas a Overpass (http://overpass-api.de/), una interfaz que permite extraer información de la base de datos global de OpenStreetMap. Overpass requiere que se especifique una "bounding box", es decir las coordenadas de un rectángulo que abarque la zona de interés.

Podemos obtener la bounding box de cualquier lugar con la función `getbb()`:


```{r}
bbox <- getbb("Rosario, Santa Fe")
bbox
```


Con `getbb()` también podemos obtener un polígono con los límites políticos, las fronteras exactas, de un lugar. Esto es muy útil para realizar mapas, o para filtrar la información que obtendremos luego para quedarnos sólo con la que corresponda a nuestra ciudad de interés, descartando la de áreas aledañas:


```{r}
bbox_poly <- getbb("Municipio de Rosario, Santa Fe", format_out = "sf_polygon")
```

Para asegurarnos de que tenemos el lugar que queremos, y no otro de nombre similar en alguna otra parte del mundo, lo verificamos en un mapa rápido provisto vía `leaflet`:


```{r eval=FALSE}
leaflet(bbox_poly) %>%
    addTiles() %>% 
    addPolygons()
```


```{r echo=FALSE}
leaflet(bbox_poly) %>% 
    addProviderTiles(provider = "OpenStreetMap") %>% 
    addPolygons()
```


Luce bien, así que continuamos.

### Extrayendo información de OSM

El siguiente paso es utilizar la función `add_osm_feature()` para especificar los datos que queremos descargar. Esto requiere conocer las palabras clave con las que se identifican los registras en la base de OSM, que permiten indicar con gran detalle el tipo de datos georeferenciados que queremos: ya sean áreas de parques públicos, posición de oficinas de correo o cajeros automáticos, vías de ferrocarril... u otro, en un larguísimo etcétera que se puede consultar en https://wiki.openstreetmap.org/wiki/Map_Features   
En este caso vamos a solicitar todas las vías de circulación (calles, avenidas, autopistas, etc) de la ciudad. En la base de datos de OSM todas aparecen con la clave _"highway"_.

```{r}
rosario <- opq(bbox) %>% 
    add_osm_feature(key = "highway")
```

Observemos que lo único que hemos obtenido hasta ahora es la definición de una consulta (qué y en dónde), pero aun no descargamos ningún dato:

```{r}
rosario
```


Es sólo la definición de una consulta a la base de datos de OpenStreetMap: "Todas las calles (objetos con clave "highway") dentro de éste rectángulo (que sabemos, corresponde a Rosario)". Para hacer efectiva la consulta y descargar los datos, la pasamos por la función `osmdata_sf()` que recolecta lo que buscamos y lo entrega en forma de dataset espacial: 


```{r}
rosario <- rosario %>% 
    osmdata_sf()
```

La descarga de información para una ciudad grande puede tomar varios minutos, y más aún la de un área metropolitana (o país, o continente, etc) así que es normal esperar un poco en ésta parte.

En cuanto se completa, ya tenemos calles:

```{r}
rosario
```

La consulta devolvió toda la información de puntos, líneas y polígonos disponibles en la base de OSM. A nos otros nos interesan las líneas, "_osm_lines_", que demarcan la traza de las calles. Los registros con otras geometrías, como polígonos, pueden representar elementos asociados a las calles como bulevares o áreas de vereda que no vamos a usar por el momento.

Del conjunto de datos disponibles, extraemos el dataframe con líneas, y chequeamos los atributos disponibles. Todos han sido recopilados por la comunidad de OpenStreetMap.

```{r}
calles <- rosario$osm_lines

head(calles) 
```

### Visualizando los resultados

Dado que las calles han sido descargadas en formato `sf`, podemos visualizarlas con `ggplot:` y `geom_sf`:


```{r}
ggplot() +
    geom_sf(data = calles)
```

Las calles exceden los límites de Rosario, ya que tenemos todos los datos encontrados dentro del rectángulo de la _bounding box_. Para "recortar" los datos conservando solo las calles de la ciudad, podemos extraer su intersección con el polígono de límites que obtuvimos antes.

```{r}
calles <- st_intersection(calles, bbox_poly)
```

Ahora si!

```{r}
ggplot() +
    geom_sf(data = calles)
```


Podemos visualizar atributos de las calles, por ejemplo el de la velocidad máxima permitida, que está presente para casi todas. Pero antes va a ser necesario limpiar un poco los datos... como es usual.

Los _dataframes_ en formato `sf` que crea `osmdata` tienen todos los valores en formato texto, incluso aquellos que son números como _maxspeed_ (la velocidad máxima), o _lanes_, la cantidad de carriles. Lo arreglamos:


```{r}
calles <- calles %>% 
  mutate(maxspeed = as.numeric(maxspeed),
         lanes = ifelse(is.na(lanes), 1, as.numeric(lanes)))
```

Con eso tenemos limpias las variables de velocidad máxima y ancho en carriles. Listos para visualizar.

```{r}
ggplot(calles) +
    geom_sf(aes(color = maxspeed), alpha = 0.5) +
    scale_color_viridis_c() +
      theme_void() +
    labs(title = "Rosario",
         subtitle = "Vías de circulación",
         caption = "fuente: OpenStreetMap",
         color = "velocidad máxima")
```

O podemos revisar la posición de las avenidas:

```{r}
ggplot() +
    geom_sf(data = calles,
             color = "gray40", alpha = .5) +
    geom_sf(data = filter(calles, str_detect(name, "Avenida")), 
            color = "salmon") +
    theme_void() +
      labs(title = "Rosario",
         subtitle = "Avenidas",
         caption = "fuente: OpenStreetMap")
```


## Un ejercicio más: ¡Bares en el barrio!


Imaginemos que estamos interesados en identificar y caracterizas los bares presentes en un barrio determinado, como San Telmo en la Ciudad de Buenos Aires. Como punto de partida, podemos consultar la base de OSM a ver que encontramos.

Comenzamos por definir nuestra área de interés

```{r eval=FALSE}
bbox_st <- getbb('San Telmo, Ciudad Autonoma de Buenos Aires')

bbox_st_poly = getbb('San Telmo, Ciudad Autonoma de Buenos Aires', format_out = "sf_polygon")

leaflet(bbox_st_poly) %>% 
  addTiles() %>% 
  addPolygons()

```


```{r echo=FALSE}
bbox_st <- getbb('San Telmo, Ciudad Autonoma de Buenos Aires')

bbox_st_poly <- getbb('San Telmo, Ciudad Autonoma de Buenos Aires', format_out = "sf_polygon")

leaflet(bbox_st_poly) %>% 
  addProviderTiles(provider = "OpenStreetMap") %>% 
  addPolygons()
```

Habiendo verificado que tenemos el área correcta, armamos una consulta por la grilla de calles, y la ejecutamos.

```{r}
SanTelmo_calles <- opq(bbox_st) %>% 
    add_osm_feature(key = "highway") %>% 
    osmdata_sf()
```

Y también descargamos información sobre la posición de bares. Habiendo revisado https://wiki.openstreetmap.org/wiki/Map_Features, sabemos que para obtener bares necesitamos la categoría "amenity", y el subtipo "bar". En términos de OSM, `key = "amenity", value = "bar"`:

```{r}
SanTelmo_bares <- opq(bbox_st) %>% 
  add_osm_feature(key = "amenity", value = "bar") %>% 
  osmdata_sf() 
```


Extraemos la información dentro de los límites exactos del barrio. 

A diferencia de las calles, que aparecen en la geometría de líneas, para los bares nos interesan los puntos.

```{r}
SanTelmo_calles <- st_intersection(SanTelmo_calles$osm_lines, bbox_st_poly)
SanTelmo_bares <- st_intersection(SanTelmo_bares$osm_points, bbox_st_poly)
```

Y listos para mapear! De paso, resaltamos aquellos donde se baila tango, al colorear según el atributo "dance.style", incluido en los datos.

```{r}
ggplot() +
  geom_sf(data = SanTelmo_calles, 
            color = "darkslateblue") +
  geom_sf(data = SanTelmo_bares, 
            aes(color = dance.style)) +
  geom_sf_label(data = SanTelmo_bares, 
                  aes(label = name), size = 2) +
  theme_void() +
  labs(title = "San Telmo",
       subtitle = "Bares",
       caption = "fuente: OpenStreetMap",
       color = "Ofrecen baile")
```

Casi listo. Antes de darnos por satisfechos, tenemos que mejorar la ubicación de las etiquetas, que se superponen por la proximidad de los lugares.

Por el momento geom_sf_label() -la geometría de ggplot que permite graficar etiquetas de datos _sf_- no incluye la útil opción de correr la posición de las etiquetas en forma automática para que no se solapen. Por suerte, existe un pequeño paquete, `ggsflabel`, que provee la funcionalidad que necesitamos.

Podemos instalar el paquete directo desde el repositorio de su autor:

```{r eval=FALSE}
install.packages("devtools")
devtools::install_github("yutannihilation/ggsflabel")
```

```{r}
library(ggsflabel)
```


Y ahora, usamos `geom_sf_label_repel()` para la versión final de nuestro mapa de bares en San Telmo:

```{r}
ggplot() +
  geom_sf(data = SanTelmo_calles, 
            color = "darkslateblue") +
  geom_sf(data = SanTelmo_bares, 
            aes(color = dance.style)) +
  geom_sf_label_repel(data = SanTelmo_bares, 
                  aes(label = name), size = 2) +
  theme_void() +
  labs(title = "San Telmo",
       subtitle = "Bares",
       caption = "fuente: OpenStreetMap",
       color = "Ofrecen baile")
```

<!--chapter:end:02-acceso_a_info_urbana.Rmd-->

## Obteniendo y analizando datos de redes sociales

## Investigando con social media

Los contenidos generados en las redes sociales son producidos y recopilados por por usuarios individuales (o representeantes de organizaciones) que participan en plataformas de acceso público tales como Twitter, Facebook, o Instagram. Si bien esas tres son las más populares, existen muchísimas otras plataformas funcionan como repositorios de información online como Yelp (reseñas de restoranes) o Properati (listados de propiedades en venta y alquiler), entre tantos otros. 

La información producida en redes sociales llama la atención de investigadores en temas sociales por el nivel de detalle que encierra. Los usuarios registran y transmiten en forma pública un amplio abanico de datos personales: su paradero, estado de ánimo, intenciones futuras, etc. Es por eso que la "minería" de datos capturados _online_ se utiliza para estudiar procesos sociales, políticos, y hasta meteorológicos ( monitoreando menciones a eventos climáticos).


### Los desafíos de trabajar con información de repositorios sociales

Por supuesto, no todo son ventajas. El análsis de datos extraidos de redes sociales se enfrenta a varios obstáculos, entre ellos:

* __"Suciedad" de los datos__. Los contenidos publicados en redes sociales suelen combinar texto, imágenes y video, lo cual requiere un esfuerzo considerable para identificar y clasificar los tipos de contenido disponibles. Incluso el contenido más fácil de tratar, el texto, requiere de limpieza previa: hay que lidiar con abreviaciones, emojis, puntuación inusual, etc. 
* __Inconsistencia__. Los regisros capturados desde repositorios online suelen ser inconsistentes: Muchas veces, uno o más de los valores de sus atributos (tales como "usario", "mensaje", "idioma", etc) faltan en muchos de los registros. Por ejemplo, algunos contienen coordenadas espaciales que permiten ubicarlos en el espacio, pero en muchos casos no están georreferenciados. Eso dificulta saber dónde está siendo producida la información, desde donde se emite.
* __Sesgo y veracidad dudosa__. Al analizar los datos, es tentador realizar inferencias acerca de lo que la población en general hace o quiere. Pero hay que tener en cuenta que las personas que producen contenidos online son un grupo particular, que tiende a ser más joven y de nivel socioeconómico mayor a la media. Por otra parte, que algo se haya dicho online dista mucho de ser cierto o sincero. Ni siquiera podemos asumir que los usuarios son individuos humanos; las redes sociales son utilizadas en creciente medida por "bots", software automático que publica contenidos en gran volumen simulando ser una persona, o un grupo de personas, con el fin de manipular la opinión pública. 

* __Volumen__. Cuando uno decide acumular los datos que obtiene de redes sociales, durante meses o años, el volumen alcanzado no es trivial. Resguardar, ordenar, clasificar y extraer sentido de  decenas o centenas de millones de registros es un desafío de big data.

* __Limitaciones de acceso__: Las empresas que controlan los repositorios de datos producidos en redes sociales vigilan con cuidado a quienes acceden, y limitan la cantidad de información que puede extraerse. En el caso de Twitter, las consultas permitidas a su base de datos se limitan al contenido producido en la última semana, y no entrega más de 18,000 tweets por consulta. 



## Conectando R a Twitter

Para acceder a los sistemas de Twitter necesitamos obtener una autorización, identificándonos con nuestro usuario en la red. Este paso es inevitable, ya que sin una autorización Twitter no responderá nuestras consultas. Por suerte, el trámite para obtener acceso es inmediato, y como resultado obtendremos una serie de códigos de identificación, conocidos en su conjunto como _API keys_. 

### Obteniendo autorización

El primer paso es, si no lo hemos hecho aún, crear un usuario de Twitter en https://twitter.com/signup. Luego seguimos los pasos de éste instructivo https://towardsdatascience.com/access-data-from-twitter-api-using-r-and-or-python-b8ac342d3efe. Nota: Twitter nos preguntará cómo se llama la "app" para la cual estamos solicitand acceso. No vamos a crear ninguna app, pero aún así tenemos que elegir un nombre; usemos "RTWEET" (aunque podría ser cualquier otro). Al completar los pasos estaremos en poder de cuatro códigos: _Consumer Key_, _Consumer Secret_, _Access Token_ y _Access Token Secret_. Tomemos nota de ellos.


### Acceso a Twitter via R: el paquete _rtweet_

`rtweet` provee un conjunto de funciones que nos facilitan interactuar con Twitter. Si no lo tenemos instalado, lo conseguimos vía: 

```{r eval=FALSE}
install.packages("rtweet")
```

Y lo activamos junto al resto de los paquetes que vamos a usar.

```{r}
library(rtweet)
library(tidyverse)
```

A continuación, le pasamos a `rtweet` los datos de autorización que conseguimos antes para crear un "token" (en la jerga de Twitter, es una especie de comprobante de que estamos autorizados a acceder a los datos)

```{r eval=FALSE}
# El nombre que le asgnamos a la app en el formulario de autorización
appname <- "RTWEET"
## consumer key (en el ejemplo no es una clave real, usar la verdadera)
consumer_key <- "la_secuencia_de_caracteres_de_nuestra_consumer_key"
## consumer secret (en el ejemplo no es un clave real, usar la verdadera)
consumer_secret <- "la_secuencia_de_caracteres_de_nuestra_consumer_secret"
## consumer key (en el ejemplo no es una clave real, usar la verdadera)
access_token <- "la_secuencia_de_caracteres_de_nuestro_access_token"
## consumer secret (en el ejemplo no es un clave real, usar la verdadera)
access_secret <- "la_secuencia_de_caracteres_de_nuestro_access_secret"



twitter_token <- create_token(
  app = appname,
  consumer_key = consumer_key,
  consumer_secret = consumer_secret,
  access_token = access_token, 
  access_secret = access_secret)
```

Al ejecutar esas líneas se abrirá una ventana en nuestro navegador solicitando autorizar el acceso vía R -lo aceptamos, por supuesto.

Ahora si, estamos listos para realizar consultas en el archivo de Twitter. La función `search_tweets()` permite obtener tweets que cumplan los requisitos que fijemos. Por ejemplo, para buscar tweets que contienen el término "inflacion", usamos:

```{r eval=FALSE}
tweets <- search_tweets(q = "inflacion", n = 3000)
```

```{r echo=FALSE}
tweets <- readRDS("data/tweets_inflacion.RDS")
```

El parámetro n = 3000 es para limitar la búsqueda a los primeros 3000 tweets hallados.

También puede hacerse una búsqueda por múltiples términos. Por ejemplo, buscando "ciudad+universitaria" hace que twitter devuelva resultados donde las palabras aparecen juntas y en ese orden; como alternativa, al optar por "ciudad universitaria" se obtienen tweets donde aparezcan esas palabras en cualquier parte del texto, sin importar su orden o si aparecen contiguas. 

El resultado es un dataframe de 3000 observaciones -el número máximo que habíamos solicitado- y 88 columnas. `rtweet` incluye `users_data()`, una función que muestra detalles de los usuarios que han producido los tweets que capturamos: 

```{r}
users_data(tweets) %>% head()
```

También podemos explorar los resultados en base a las 88 variables disponibles. Revisemos los nombres:

```{r}
names(tweets)
```


Allí hay de todo para explorar.

### Usuarios más populares

Según la cantidad de seguidores:

```{r}
options(scipen = 20)
ggplot(tweets) +
    geom_histogram(aes(x = followers_count))
```

El gráfico muestra una distribución de ["power law"](https://es.wikipedia.org/wiki/Ley_potencial), típica en los rankings de popularidad. Hay una enorme masa de usuarios con popularidad mínima (apenas un puñado de seguidores) y un número muy pequeño de usuarios que alcanza una cantidad deseguidores cientos o miles de veces superior a la de la mayoría.


Obtenemos un top 5 de los usuarios más populares (con más seguidores), su procedencia, y el contenido del tweet:

```{r}
tweets %>% 
    top_n(5, followers_count) %>% 
    arrange(desc(followers_count)) %>% 
    select(screen_name, followers_count, location, text)
```

### Tweets más populares

En base a la cantidad de retweets que recibieron. Nos quedamos sólo con los tweets originales, descartando los que son retweets en si mismos ("is_retweet == TRUE"), y revisamos la distribución de sus retweets:


```{r}
ggplot(filter(tweets, !is_retweet))+
    geom_histogram(aes(x = retweet_count))
```


Otra ditribución _power law_. Identifiquemos el tweet original más que sumó más retweets:

```{r}
tweets %>% 
    filter(!is_retweet) %>% 
    filter(retweet_count == max(retweet_count)) %>% 
    select(screen_name, retweet_count, followers_count, location, text)
```


Nota: Si no estamos interesados en capturar retweets, podemos evitarlos al momento de consultar la base de Twitter, as'i `tweets <- search_tweets(q = "inflacion", n = 500, include_rts = FALSE)`

### La hora del día a la que se producen los tweets

`rtweet()` provee una funció que hace facil mostrar la evolución temporal de nuestros tweets: `ts_plot()`. Podemos ver la frecuencia de tweets por segundo, hora, día, semana, mes o año eligiendo el parámetro correspondiente ("secondss", "minutess", "hours", "days", "weeks", "months", o "years")

```{r}
ts_plot(tweets, "minutes")
```

### Procedencia de los usuarios


```{r}
tweets %>%
  ggplot() +
  geom_bar(aes(location)) + 
    coord_flip() +
     labs(title = "Procedencia de los usuarios",
          x = "cantidad",
          y = "ubicación")
```

Dado que el campo "location" refleja el texto que cada usuario eligió para describir su ubicación (no se trata de las coordenadas de origen del tweet) las variabilidad es grande. Algunas escriben su país, otros su ciudad, otras su barrio... y hay quienes eligen opciones poéticas cómo "algún lugar del mundo". En todo caso, la abundancia de opciones resulta en un gráfico muy difícil de leer. 

Probamos extraer el top 10 de lugares más frecuentes, eliminando los tweets de usuarios sin datos en su atributo "location".


```{r}
tweets %>%
    filter(location != "", !is.na(location)) %>% 
    count(location) %>% 
    top_n(10, n) %>% 
    ggplot() +
      geom_col(aes(x = reorder(location, n), y = n)) + 
      coord_flip() +
      labs(title = "Procedencia de los usuarios",
           x = "ubicación",
           y = "cantidad")
```


## Escuchando tweets en tiempo real

Como alternativa a consultar el archivo "histórico" de Twitter, es posible conectar a su API de streaming, que entrega tweets en tiempo real al instante en que se producen. La función `stream_tweets()` permite iniciar una conección y capturar tweets hasta concluya el tiempo dispuesto por el parámetro "timeout", expresado en segundos.

Por ejemplo, para "escuchar" el stream de Twitter por un minuto (60 segundos), y capturar mensajes que incluyan los  términos _accidente_ y _tránsito_: 

```{r eval=FALSE}
captura_streaming <- stream_tweets(q = "accidente+tránsito", timeout = 60)
```


```{r echo=FALSE}
captura_streaming <- readRDS("data/tweets_streaming.RDS")
```


Verificamos el resultado (sólo los campos de usuario y texto del tweet):

```{r}
captura_streaming[4:5]
```


## Capturando tweets por períodos prolongados

Cuando queremos monitorear un evento de actualidad, por ejemplo capturando tweets que mencionen una palabra o hashtag de interés, resulta necesario mantener las escucha activa durante varias horas o días. La solución para este caso es usar la función `stream_tweets()`, que permite iniciar un proceso de escucha de tiempo arbitrario. Dado que no se sabe que puede fallar en un proceso que dura varios días, la función se encarga de guardar los resultados en un archivo local a medida que se obtienen, y reiniciar la conexión a Twitter en forma automática si se interrumpe por algún motivo (como un corte momentáneo de acceso a internet). 

La usamos así:

```{r eval=FALSE}

terminos_de_busqueda <- "accidente + tránsito"

# una semana: 60 segundos * 60 * 24 * 7
tiempo <- 60 * 60 * 24 * 7

# El archivo donde guardar los resultados en disco (tendrá formato json, así que lo usamos en el nombre de archivo)
archivo <- "busqueda_tweets.json"
    

stream_tweets(q = terminos_de_busqueda,
              timeout = tiempo,
              file_name = archivo,
              parse = FALSE)
```
Una vez que el período de captura termina, podemos leer el archivo generado.

```{r eval=FALSE}
# en el paso anterior definimos que el nombre de archivo es "busqueda_tweets_DT_VP.json"
tweets <- parse_stream("busqueda_tweets.json")
```

Y con eso concluye el proceso. Ya estamos listos para analizar el contenido.


## Capturando tweets en zonas específicas

Imaginemos ahora que queremos obtener tweets originados en un lugar en particular. En un barrio, una ciudad, o un país. Para ello podemos aprovechar que Twitter permite realizar búsquedas por área geográfica.

Por ejemeplo, iniciemos la descarga de tweets que mencionen el nombre que se le da en Buenos Aires al metro:  "subte". La clave está en que además de los términos de búsqueda vamos a especificar un radio de 20 millas (~32 km) en torno al área céntrica (el _downtown_) de la Ciudad:

```{r eval=FALSE}
tweets_transporte <- search_tweets(q = "subte",
              geocode = "-34.603722,-58.381592,20mi",
              include_rts = FALSE,
              n = 100000,
              retryonratelimit = TRUE)

```

```{r echo=FALSE}
tweets_transporte <- readRDS("data/tweets_transporte.RDS")
```

El proceso puede tomar un rato. Quien no pueda esperar, puede descargar unos resultados obtenidos previamente:

```{r eval=FALSE}
tweets_transporte <- readRDS(url("https://bitsandbricks.github.io/data/tweets_transporte.RDS"))
```

### Extraer las coordenadas

Algunos de los tweets, aquellos que fueron publicados desde un teléfono móvil u otro dispositivo con GPS, tienen coordenadas de posición precisas (latitud y longitud). El dataframe creado por `rtweet` guarda la pareja de coordenadas em el campo "coords_coords", dentro de una lista. Es decir que en lugar de un valor simple, cada elemento de la columna contiene una lista de varios valores. De manera similar, también crea otras dos columnas, "geo_coords" y "bbox_coords" que contienen datos sobre la ubicación del tweet en forma de lista.

Esto trae dos problemas:

* No podemos usar los verbos de manipulación de datos con esas columnas (filter, mutate, arrange, etc) porque están diseñados para trabajar con valores atómicos, como "hola" y no listas, como ("hola", "chau", "sin datos").

* No podemos guardar el dataframe en formato .csv, el favorito de los que comparten datos, porque no hay forma estandarizada de indicar que algunos campos contienen una lista de datos en lugar de un valor único. `write.csv` y `write_csv` intentan guardar el dataframe en un archivo .csv, pero fallan al encontrar la primera columna que contiene listas.

La solución es simple: usamos la función `lat_lng()`, que agrega al dataframe dos columnas adicionales llamdas "lat" y "lng",  conteniendo latitud y longitud para los tweets que traen posición exacta.

```{r}
tweets_transporte <- lat_lng(tweets_transporte)
```


Además, si quisiéramos guardar luego los datos en formato _.csv_, podemos descartamos los campos problemáticos -los que contienen información geográfica en forma de listas:

```{r}
tweets_transporte <- tweets_transporte %>% 
    select(-geo_coords, -coords_coords, -bbox_coords) 
```

### Visualizando los datos georeferenciados en un mapa interactivo


Para empezar, filtramos nuestros tweets para conservar sólo los que contienen coordenadas exactas de posición.

```{r}
tweets_transporte_geo <- tweets_transporte %>% 
    filter(!is.na(lat), !is.na(lng))
```

El resultado evidencia que los tweets georeferenciados son sólo una fracción del total que se produce:

```{r}
nrow(tweets_transporte_geo)
```

Ahora vamos a llearlos a un mapa.


## Mapas estáticos con ggmap

`ggmap` es un paquete de R que complementa a ggplot, agregando funciones que permiten adquirir y visualizar mapas en forma fácil.

Si no lo tenemos instalado, ya sabemos que hacer:

```{r eval=FALSE}
install.packages("ggmap")
```

Lo activamos:

```{r}
library(ggmap)
```

Ahora, para obtener un mapa base del área donde se encuentran los puntos que queremos mostrar, necesitamos determinar la "bounding box": el rango de latitud y longitud que forma un rectángulo conteniendo todas las posiciones. En resumidas cuentas, se trata de los valores de latitud máxima y mínima, y de longitud máxima y mínima.

Los proveedores de mapas online suelen solicitar los valores en este orden: izquierda, abajo, derecha, arriba. Es decir, posición mas al oeste, posición mas al sur, posición mas al este, posición mas al norte.

Cuando disponemos de un dataframe con columnas de latitud y longitud, obtener la _bounding box_ es bastante fácil:

```{r}
bbox <- make_bbox(lon = tweets_transporte_geo$lng, lat = tweets_transporte_geo$lat)

bbox
```

Con eso podemos descargar un mapa del área. Como opción por defecto, `ggmap` solicita los mapas a Google Maps, pero ésta ha dejado de ser la alternativa ideal: desde octubre de 2018, Google exige a los usarios registrarse y proveer una tarjeta de crédito para descargar información mediante porgramas propios. Por eso vamos a usar otra de las fuentes habilitadas por `ggmap`, el servicio de mapas de [Stamen Design](http://maps.stamen.com).


Lo descargamos entregando la bounding box del área que nos interesa y un nivel de zoom. El nivel de zoom -un número de 1 a 20- indica el detalle que tendrá el mapa descargado. Para un área metropolitana un zoom de entre 10 y 12 es adecuado.  

```{r cache=TRUE}
mimapa <- get_stamenmap(bbox, zoom = 11)
```

Para ver el resultado usamos `ggmap()`:

```{r}
ggmap(mimapa)
```

Stamen ofrece varios estilos de mapa: "terrain" (usado por defecto), "terrain-background", "terrain-labels", "terrain-lines", "toner", "toner-2010", "toner-2011", "toner-background", "toner-hybrid", "toner-labels", "toner-lines",
"toner-lite", "watercolor".

Probemos algunos:


```{r cache=TRUE}
mimapa_terrain_lines <- get_stamenmap(bbox, maptype = "terrain-lines", zoom = 11)
mimapa_toner_lite <- get_stamenmap(bbox, maptype = "toner-lite", zoom = 11)
mimapa_watercolor <- get_stamenmap(bbox, maptype = "watercolor", zoom = 11)

ggmap(mimapa_terrain_lines)
ggmap(mimapa_toner_lite)
ggmap(mimapa_watercolor)
```

Cuando descargamos un mapa que vamos a usar de base para visualizar datos, siempre es una buena idea elegir una opción en escala de grises, sin colores que compitan contra los datos que proyectaremos. Probamos entonces con "toner-lite" para el mapa que usaremos de aqui en adelante.

```{r cache=TRUE}
mapa_BA <- get_stamenmap(bbox, maptype = "toner-lite", zoom = 11)

ggmap(mapa_BA)
```

 
Ahora agregamos capas de puntos mostrando la posición de los tweets. La sintaxis es la misma que aprendimos para `ggplot`; de hecho, `ggmap` es una llamada a `ggplot` que tras bambalinas se encarga de los ajustes necesarios para mostrar el mapa como findo. Habiendo revisado la data de Moreno, sabemos que las columnas de longitud y latitud de los puntos georeferenciados se llaman "lon" y "lat". Al graficar los puntos, las usaremos como posición x e y respectivamente.


```{r}
ggmap(mapa_BA) +
    geom_point(data = tweets_transporte_geo, aes(x = lng, y = lat))
```

También podemos asignar a cada punto un color de acuerdo a la popularidad del usuario:

```{r}
ggmap(mapa_BA) + 
    geom_point(data = tweets_transporte_geo, 
               aes(x = lng, y = lat, color = followers_count)) +
    scale_color_distiller(palette = "Spectral")
```

¿Qué pasó aquí? Tenemos un escala de colores que llega hasta 600.000, en el tono rojo, pero en el mapa solo vemos puntos azules, los que indican una cantidad baja de seguidores. La explicación está en la relativa rareza de usuarios de Twitter con cientos de miles de seguidores. Dado que la inmensa mayoría de usuarios de la red sólo tienen un puñado de seguidores, ocurre que puntitos que los representan suelen tapar a los esporádicos usuarios ultra populares. Si lo que queremos es mostrar los tweets de éstos últimos, podemos recurrir a un pequeño truco. Dado que las filas de un data rame se grafican en orden, si ordenamos las observaciones en orden creciente de "followers_count" los usuarios populares serán graficados al final, garantizando que su color aparezca por encima de otros.

```{r}
tweets_transporte_geo <- arrange(tweets_transporte_geo, followers_count)

ggmap(mapa_BA) + 
    geom_point(data = tweets_transporte_geo, 
               aes(x = lng, y = lat, color = followers_count)) +
    scale_color_distiller(palette = "Spectral")
```

Tambien podemos usemos usar el tamaño de cada punto para representar la repercusión de los tweets, en base a la cantidad de "retweets" que han obtenido:

```{r}

ggmap(mapa_BA) + 
    geom_point(data = tweets_transporte_geo, 
               aes(x = lng, y = lat, color = followers_count, size = retweet_count),
               alpha = .5) +
    scale_color_distiller(palette = "Spectral")

```



### Mapas interactivos con leaflet

Con la explosión de de popularidad de los mapas online, con Google Maps al frente, se ha vuelto habitual explorar información geográfica en entornos interactivos, que permiten al usuario desplazarse libremente por la superficie terrestre y cambiar el nivel de zoom con el que se muestran los datos. Un mapa con información tan precisa como la posición de los tweets, que incluso permite ver a parcela desde donde se han emitido, se beneficia en extremo de la posibilidad de variar la escala de visualización a voluntad.

Desde R es fácil proyectar nuestros datos sobre mapas interactivos, usando el paquete `leaflet`. Si aún no lo tenemos en nuestro sistema, lo obtenemos mediante:

```{r eval=FALSE}
install.packages("leaflet")
```

Una vez que está instalado, lo activamos

```{r}
library(leaflet)
```


EL uso de leaflet es similar al de `ggplot`; uno toma un dataframe y lo muestra mediante capas que exponen distintos aspectos de la información. Para empezar, hacemos

```{r}
leaflet(tweets_transporte_geo) 
```

... y no obtuvimos mucho. Tal como pasa con `ggplot()`, si uno no define ninguna capa de visualización, el resultado es una especie de lienzo vacío.

Siguiente paso: agregar un mapa base. Para sumar capas a un mapa de `leaflet` usamos " %>% " en ugar del " + " que requiere `ggplot()`, pero el concepto es el mismo.

```{r eval=FALSE}
leaflet(tweets_transporte_geo) %>% 
    addTiles() 
```


```{r echo=FALSE}
leaflet(tweets_transporte_geo) %>% 
    addProviderTiles(providers$OpenStreetMap)
```

Ahora está un poco mejor, nos encontramos con un mapa, pero falta que aparezcan nustros datos. Es fácil: con `addMarkers()` leaflet se encarga de buscar las coordenadas de cada observación, y si aparecen con algún nombre esperable, las identifica y sitúa en el mapa un pin por cada una.

Nombres esperables serían "latitude" y "longitude" o también, como en nuestro caso, "lat" y "lng". Si las coordenadas aparecieran bajo columnas con nombres menos interpretables, se le puede aclarar a `leaflet` cuáles son vía paramétros.

```{r eval=FALSE}
leaflet(tweets_transporte_geo) %>% 
    addTiles() %>%
    addMarkers()
```


```{r echo=FALSE}
leaflet(tweets_transporte_geo) %>% 
    addProviderTiles(providers$OpenStreetMap) %>%
    addMarkers()
```

Ya tenemos un mapa útil! Para mejorarlo, agregamos la opción de "popup", que permite extraer información adicional cliqueando sobre un pin. Por ejemplo, el contenido del campo con el texto de cada tweet (nótese el uso de "~", que leaflet requiere para entender que nos referimos a un campo presente en el dataframe que le pasamos).

```{r eval=FALSE}
leaflet(tweets_transporte_geo) %>% 
    addTiles() %>% 
    addMarkers(popup = ~text)
```


```{r echo=FALSE}
leaflet(tweets_transporte_geo) %>% 
    addProviderTiles(providers$OpenStreetMap) %>%
    addMarkers(popup = ~text)
```


Para sumar una dimensión más a la visualización, podemos usar el color para indicar la cantidad de seguidores del autor de cada tweet. Para codificar por color, `leaflet` requiere definir una paleta de colores para aplicar a nuestros datos. Al crear una paleta debemos elegir la función correspondiente al tipo de datos que vamos a mostrar: `colorFactor()` para variables categóricas, `colorNumeric()` para variabes numéricas, o `colorQuantile()` también para variables numéricas, pero agrupadas en cuantiles. La función requere al menos dos parámetros. Uno es "palette", para definir los tonos a usar. Aquí funcionan nuestros amigos _viridis_, _magma_, _plasma_ e _inferno_, y también las [paletas Brewer](https://data.library.virginia.edu/files/pal_fig_3.png), como _"Spectral_ o _Accent_). El parametro restante es "domain", que simplemente toma un vector con los datos que vamos a representar con la paleta.

Como la cantidad de seguidores es una variable numérica, podemos usar:

```{r}

paleta <- colorNumeric(
  palette = "viridis",
  domain = tweets_transporte_geo$followers_count)

```

Y luego la usamos en nuestro mapa:

```{r eval=FALSE}
leaflet(tweets_transporte_geo) %>% 
    addTiles() %>% 
    addCircleMarkers(popup = ~text,
                     color = ~paleta(followers_count))
```

```{r echo=FALSE}
leaflet(tweets_transporte_geo) %>% 
    addProviderTiles(providers$OpenStreetMap) %>% 
    addCircleMarkers(radius = ~retweet_count,
                     popup = ~text,
                     color = ~paleta(followers_count))
```
Como siempre es muy util agregar una leyenda que explique la codificación de los datos. `leaflet` sólo permite mostrar leyendas basadas en color (no en el diamétro de los círculos), pero algo es algo. Agregamos la leyenda así:

```{r eval=FALSE}
leaflet(tweets_transporte_geo) %>% 
    addTiles() %>% 
    addCircleMarkers(radius = ~retweet_count,
                     popup = ~text,
                     color = ~paleta(followers_count)) %>% 
    addLegend(title = "seguidores", pal = paleta, values = ~followers_count)

```


```{r ultimo_chunk_redes_sociales, echo=FALSE}
leaflet(tweets_transporte_geo) %>% 
    addProviderTiles(providers$OpenStreetMap) %>%
    addCircleMarkers(radius = ~retweet_count,
                     popup = ~text,
                     color = ~paleta(followers_count)) %>% 
    addLegend(title = "seguidores", pal = paleta, values = ~followers_count)

```


Esto es sólo una introducción a la producción de mapas interactivos. 
Para un recorrido por muchas otras opciones disponibles con `leaflet`, visitar https://rstudio.github.io/leaflet/

<!--chapter:end:03-redes_sociales.Rmd-->

# Geoprocesamiento

ESCRIBIR INTRO

## Cruces espaciales 

Hay ocasiones en que necesitamos cruzar datos de fuentes distintas en base a su ubicación geográfica. Es decir, un “join” que cruce registros en base a sus coordenadas espaciales, en lugar de otros atributos.

Aquí va un ejemplo como guía para realizar el spatial join, o join espacial, que sólo puede ser realizado entre dataframes de tipo espacial.

Paquetes que vamos a usar:


```{r eval=FALSE}
library(tidyverse)
library(sf)
```

## Dataframes tradicionales y dataframes espaciales

Vamos a trabajar con dos datasets.

Uno contiene los alojamientos ofrecidos por Airbnb en Buenos Aires en Julio 2017.


```{r primer_chunk_de_geoproc}
airbnb <- read.csv("https://query.data.world/s/55amvafrknrgkeyeiu54yb2c6u6brc",
                   stringsAsFactors = FALSE)
names(airbnb)

```

Y el otro contiene los polígonos de las comunas porteñas:

```{r}
comunas <- st_read('https://bitsandbricks.github.io/data/CABA_comunas.geojson')
```

Notemos que tenemos dos tipos de dataframe distintos. El de Airbnb es un dataframe “tradicional”, dado que todas sus columnas contiene valores simples: un número, un texto, un factor, etc.

El dataframe de comunas es especial porque es “espacial”. Contiene una columna distinta a las demás, llamada “geometry” que en lugar de una observación simple contiene una lista con múltiples posiciones. Estas posiciones son los vértices que definen el polígono de cada comuna, y permiten la proyección en mapas y el cálculo de estadísticas espaciales.

### Combinando datasets con información espacial

Si lo único que queremos es visualizar en forma combinada la información que contienen, no hay problema en que un dataframe sea espacial y otro no, siempre y cuando éste último incluya una columna con latitud y otra con longitud para identificar la posición de cada registro.

Dado que los datos de Airbnb incluyen lat/long, es fácil visualizarlos en conjunto con el dataframe espacial de las comunas:


```{r}
ggplot() +
    geom_sf(data = comunas) +
    geom_point(data = airbnb, 
               aes(x = longitude, y = latitude),
               alpha = .3, 
               color = "orange")
```


Dicho esto, si lo que queremos es combinar la información para su análisis cuantitativo, no nos alcanza con la visualización. Lo que tenemos que hacer es un “join espacial”, la técnica que permite cruzar datasets en base a sus atributos de ubicación geográfica.

Sólo es posible hacer joins espaciales entre dataframes espaciales. Es por eso que los datos de Airbnb, así como están, no sirven para un join. ¡Pero! una vez más, dado que incluyen columnas de latitud y longitud, la solución es fácil. Podemos usar las columnas de lat/long para convertirlo en un dataset espacial hecho y derecho, así:

```{r}
airbnb <- airbnb %>% 
    filter(!is.na(latitude), !is.na(longitude)) %>% 
    st_as_sf(coords = c("longitude", "latitude"), crs = 4326)
```

Tres cosas importantes a tener en cuenta:

1. Un dataframe espacial no permite filas sin posición (sin coordenadas). Por eso antes de la conversión usamos filter(!is.na(latitude), !is.na(longitude)) para descartar los registros sin coordenadas del dataset de origen si los hubiera.
2. La función st_as_sf() es la que toma un dataframe común y lo transforma en uno espacial. Con el parámetro coords = c("longitude", "latitude") le definimos como se llaman las columnas de longitud y latitud, en ese orden. Obsérvese que toma los nombres entre comillas.
3. El último parámetro, “crs”, es obligatorio y requiere el identificador del sistema de referencia de las coordenadas. Cuando se trata de datos capturados en internet (como aquí, por scraping del sitio de Airbnb), el crs siempre es 4326.
Ahora que ambos dataframes son de tipo espacial, ambos se grafican con geom_sf()

```{r}
ggplot() +
    geom_sf(data = comunas) +
    geom_sf(data = airbnb, color = "orange", alpha = .3)
```


y más importante aún, se pueden combinar con un join espacial. La versión más simple, que combina atributos de las filas cuyas posiciones coinciden en el espacio, es así:

```{r}
airbnb_con_comunas <- st_join(airbnb, comunas)
```

El resultado es un dataframe con datos de Airbnb, que en cada fila incluye los datos de la comuna con la que coincide el alojamiento:

```{r}
head(airbnb_con_comunas)
```

Con los atributos adicionales, podemos realizar sumarios por comuna de los alojamientos:

```{r}
airbnb_con_comunas %>% 
    group_by(comunas) %>% 
    summarise(cantidad = n())
comunas
```

El resultado de un join espacial también es un dataframe espacial, así que podemos visualizarlo de la manera habitual (y ahora tenemos más variables para graficar).

```{r}
ggplot() +
    geom_sf(data = comunas) +
    geom_sf(data = airbnb_con_comunas, aes(color = comunas))
```


### Coropletas

Explicar como usar los conteos de puntos por polígono para mapear en coropletas

## Uniones

Dessarrollar

## Intersecciones

Desarrollar

## Distancias

Desarrollar

## Cálculo de extensión (longitud, área)

Desarrollar

<!--chapter:end:04-geoprocesamiento.Rmd-->

# Analizando dinámicas espacio-temporales

Para entender datasets con datos en gran volumen que poseen atributos de posición y tiempo, es útil visualizar el ritmo en el que ocurren (diario, mensual, anual, etc) y la forma en la que se disrtibuyen en el espacio.

Para prácticar, trabajaremos con un dataset de delitos registrados en la Ciudad de Buenos Aires. Los datos fueron publicados por el Gobierno de la Ciudad como parte de http://mapa.seguridadciudad.gob.ar/, y recopilados en un repositorio descargable por @orramiro en https://github.com/ramadis/delitos-caba.


```{r cache=TRUE}
delitos <- read.csv("https://bitsandbricks.github.io/data/crimenydelito.csv", stringsAsFactors = FALSE)
```

Chequeamos un resumen del contenido del dataset:

```{r}
summary(delitos)
```

Mirando el resumen nos enteramos de que la hora más habitual para un delito son las 8 de la noche, que el delito más frecuente es el hurto sin violencia, y que el lugar más habitual donde ocurren los incidentes es en la vía pública, entre otras cosas.

### Trabajando con fechas

La fecha es un tipo de dato que puede ser expresado de muchas maneras, dependiendo de que nos interese teer en cuenta el día de la semana al que corresponde, el mes, el año, etc. El paquete `lubridate` hace facil extraer de fechas en cualquier formato (por ejemplo "20/07/2018") el atributo relacionado que deseemos (como "viernes" o "Julio").

Para empezar, convertimos el campo "fecha" al tipo de dato especializado que se llama... fecha (_date_). Aquí tenemos que prestar atención al formato en que aparecen, en general algo como  "2018-07-21" (mes, día y año) o "2018-07-21 12:14:24" (mes, día, año y hora, minutos, segundos). Con nuestros datos se da el primer caso, por lo cual la función para convertir ese campo en fecha es `ymd()`; para el segundo caso, seria `ymd_hms()`

```{r}
library(tidyverse)
library(lubridate)

delitos <- delitos %>% mutate(fecha = ymd(fecha))
```


Repasemos algunas de los nuevos trucos que podemos hacer con el tiempo. Tomemos cinco fechas elegidas al azar:

```{r}
set.seed("99")
muestra_de_fechas <- delitos %>% 
    mutate(fecha_hora = paste(fecha, hora)) %>% 
    sample_n(5) %>% 
    pull(fecha_hora)

muestra_de_fechas
```

Mediante las funciones diponibles en lubridate, podemos extraer:

- El día de la semana al que corresponde cada fecha:

```{r}
wday(muestra_de_fechas)
```

```{r}
wday(muestra_de_fechas, label = TRUE)
```

- El mes:

```{r}
month(muestra_de_fechas)
```


```{r}
month(muestra_de_fechas, label = TRUE)
```

- El año:

```{r}
year(muestra_de_fechas)
```

Y varias opciones más, que se pueden repasar en https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html


Con lo visto hasta aquí, tenemos suficiente para mostrar patrones temporales en los datos.

Delitos registrados por año:

```{r }
options(scipen = 20)

ggplot(delitos) + 
    geom_bar(aes(x = year(fecha)))

```

Los resultados no permiten comparar entre años, ya que el dataset tiene apenas un puñado de registros en el 2015, y sólo llega a mediados del 2017. Lección: quedémonos sólo con el 2016 y miremos dentro.

```{r}
delitos %>% 
    filter(year(fecha) == 2016) %>% 
    ggplot() +
        geom_bar(aes(x = month(fecha, label = TRUE)))
```

Se ve bastante parejo! Quizás haya que examinar los delitos por tipo, para ver si hay algunos que muestran altibajos según la estación. Veamos el top 5 de delitos por frecuencia.

```{r}
delitos %>% 
    count(tipo_delito) %>% 
    top_n(5) %>% 
    arrange(desc(n))
```

Luce razonable. Guardamos la lista de delitos más frecuentes para referenciar luego.

```{r}
delitos_frecuentes <- delitos %>% 
    count(tipo_delito) %>% 
    top_n(5) %>% 
    pull(tipo_delito)
```

Y ahora los comparamos. En gráfico de barras "apilado":

```{r}
delitos %>% 
    filter(year(fecha) == 2016,
           tipo_delito %in% delitos_frecuentes) %>% 
    ggplot() +
        geom_bar(aes(x = month(fecha, label = TRUE), fill = tipo_delito))
```

... y sin apilar

```{r}
delitos %>% 
    filter(year(fecha) == 2016,
           tipo_delito %in% delitos_frecuentes) %>% 
    ggplot() +
        geom_bar(aes(x = month(fecha, label = TRUE), fill = tipo_delito),
                 position = "dodge")
```

O como líneas:

```{r}
# Primero realizamos un conteo de delitos por tipo y por mes del año
conteo <-  delitos %>% 
    filter(year(fecha) == 2016,
           tipo_delito %in% delitos_frecuentes) %>% 
    count(tipo_delito, mes = month(fecha, label = TRUE))
   
# Y ahora a mostras las cantidades mensuales como líneas 
ggplot(conteo) +
    geom_line(aes(x = mes, y = n, group = tipo_delito, color = tipo_delito))
```

Ésta última opción es sin dudas la más clara, tanto para mostrar la diferencia relativa en el volumen de incidentes, como para indicar si existen fluctuaciones. 

Intentémoslo otra vez, ahora con el día de la semana:

```{r}
# Primero realizamos un conteo de delitos por tipo y por día de la semana
conteo <-  delitos %>% 
    filter(year(fecha) == 2016,
           tipo_delito %in% delitos_frecuentes) %>% 
    count(tipo_delito, diasemana = wday(fecha, label = TRUE))
   
# Y ahora a mostras las cantidades mensuales como líneas 
ggplot(conteo) +
    geom_line(aes(x = diasemana, y = n, group = tipo_delito, color = tipo_delito))

```

Como algunas categorías están mucho menos representadas, y quedan "aplastadas" en el gráfico con lo que se dificulta su legibilidad. Vamos a comparar porcentajes en lugar de valores absolutos.

```{r}
conteo <-  conteo  %>% 
    group_by(tipo_delito) %>% 
    mutate(pct = n / sum(n) * 100)

ggplot(conteo) +
    geom_line(aes(x = diasemana, y = pct, group = tipo_delito, color = tipo_delito))

```

La diferencia de volumen según el día parece drástica, pero es engañosa: el eje de las $y$ no empieza en 0, lo cual hace que las diferencias se perciban mayores de lo que son. Forzamos al gráfico a comenzar desde 0 el eje $y$:

```{r}
ggplot(conteo) +
    geom_line(aes(x = diasemana, y = pct, group = tipo_delito, color = tipo_delito))+
    expand_limits(y = 0)
```


Sin dudas, los domingos son el día en que el delito descansa un poco... excepto pra quienes deciden llevarse un auto o al menos una rueda ajena. El pico de delitos reportados ocurre los viernes, con la excepción del hurto de rueda, que crece de domingo a miércoles y luego decae.

Recordando que los homicidios suelen tener un contexto muy distinto al de los robos, agreguemos la categoría "Homicidio Doloso" para comparar con las demás.

```{r}
conteo_homicidios <-  delitos %>% 
    filter(year(fecha) == 2016,
           tipo_delito == "Homicidio Doloso") %>% 
    count(tipo_delito, diasemana = wday(fecha, label = TRUE)) %>%
    group_by(tipo_delito) %>% 
    mutate(pct = n / sum(n) *100)

```

Sumamos la nueva categoría:

```{r}
ggplot(conteo) +
    geom_line(aes(x = diasemana, y = pct, group = tipo_delito, color = tipo_delito)) +
    geom_line(data = conteo_homicidios,
              aes(x = diasemana, y = pct, group = tipo_delito)) +
    labs(title = "Distribución diaria por tipo de delito",
         subtitle = "La línea negra representa homicidios",
         x = "día", y = "%",
         color = "Delitos más frecuentes") +
    expand_limits(y = 0)

```

La categoría homicidio muestra un ritmo inverso al de los otros delitos: es más frecuente durante el fin de semana, decayendo en los días hábiles.


También podemos evaluar el ritmo según la hora del día. Para ello necesitamos pasar a formato temporal la columna "hora", que en éste dataset tiene el formato "hh:mm:ss" (por ejemplo, "14:55:00"). La función correspondiente para interpretar ese formato es `hms()`. Usamos una combinacion de `hms()` para interpretar el texto en "hora" como una variable de tipo tiempo, y `hour()` para extraer la hora base -por ejemplo, para "19:55:00" la hora base es 19.

```{r}
por_hora <-  delitos %>% 
    filter(year(fecha) == 2016,
           tipo_delito %in% delitos_frecuentes) %>% 
    count(tipo_delito, hora_base = hour(hms(hora))) %>%
    group_by(tipo_delito) %>% 
    mutate(pct = n / sum(n) *100)
```

```{r}
ggplot(por_hora) +
    geom_line(aes(x = hora_base, y = pct, group = tipo_delito, color = tipo_delito)) +
    labs(title = "Distribución horaria por tipo de delito",
         x = "hora", y = "%",
         color = "Delitos más frecuentes") +
    expand_limits(y = 0) +
    scale_x_continuous(breaks = 0:23)
```

Para los delitos relacionados con automóviles, el "prime time" es como el de la TV, de 20 a 22. Los robos violentos tienen su apogeo durante las 20. Las lesiones viales comparten su hora pico con el tráfico.  Los hurtos tienen su momento cúlmine a la hora del almuerzo, y a la de salida de la oficina.

## Mirando al espacio

Pasemos a hora al análisis espacial de nuestros datos. Para facilitar la visualización vamos a usar el paquete `ggmap`, que incluye varias funciones que facilitan la creación de mapas.

```{r}
library(ggmap)
```

### Obteniendo un mapa base

Para obtener un mapa de fondo, obtenemos el "bounding box" de nuestros datos, y luego se los pasamos a `get_stamenmap()`. Para más detalles sobre la descarga de mapas base, ver http://rpubs.com/HAVB/mapeo_tweets.


```{r}
delitos <- delitos %>% 
    filter(latitud <0, longitud <0)

bbox <- c(min(delitos$longitud, na.rm = TRUE),
          min(delitos$latitud, na.rm = TRUE),
          max(delitos$longitud, na.rm = TRUE),
          max(delitos$latitud, na.rm = TRUE))

CABA <- get_stamenmap(bbox = bbox, 
                      maptype = "toner-lite")
```

Para verlo:

```{r dpi=150}
ggmap(CABA)
```

### De coordenadas al mapa

De aquí en más podemos suporponer nuestros datos en distintas capas, con la misma sintaxis que conocemos de ggplot. Para mapear las ubicaciones de los delitos en el dataset, usamos `geom_point()` y los campos de longitud y latitud para los ejes $x$ e $y$:

```{r dpi=150}
ggmap(CABA) +
    geom_point(data = delitos, aes(x = longitud, y = latitud))
```

Aquí nos topamos con un problema, habitual al trabajar con grandes volúmenes de datos. Hay tantos puntos proyectados sobre el mapa, que se hace imposible interepretar donde existen más o menos. Hacemos algunso ajustes: un color más llamativo, un tamaño de punto más pequeño, y aplicación de una ligera transparencia, vía los atributos "color", "size" y "alpha". ¿Cuál es el valor ideal para cada uno? En general, no queda otra que recurrir a la prueba y error para encontrar la receta justa.

```{r dpi=150}
ggmap(CABA) +
    geom_point(data = delitos, aes(x = longitud, y = latitud),
               color = "orange", size = 0.1, alpha = 0.1)
```

Ahora si aparecen ciertos patrones, por ejemplo la afinidad del delito con las grandes vías de circulación de la ciudad. Aún así, se hace dificil identificar de un golpe de vsta las "zonas calientes", los puntos de máxima concentración. 

### Mapas de densidad

Una solución práctica para el problema de la cantidad de puntos es una técnica llamada "binning": dividir el espacio en una grilla de celdas, contar cuantos puntos caen dentro de cada una, y visualizar las cantidades agregadas. Hacerlo es muy fácil vía `geom_bind2d()`.

```{r dpi=150}
ggmap(CABA) +
    geom_bin2d(data = delitos, 
               aes(x = longitud, y = latitud))
```

Ahora si, resaltan las áreas de mayor concentración de incidentes. Se puede mejorar un poco el gráfico usando una mayot cantidad de celdas para aumentar la resolución. También empleando una escala de colores diseñada para ayudar a detectar diferencias por tonalidad, como viridis.

```{r dpi=150}
ggmap(CABA) +
    geom_bin2d(data = delitos, aes(x = longitud, y = latitud), bins = 100) +
    scale_fill_viridis_c()
```

Una alternativa al _binning_ es la llamada _kernel density estimation_, muy utilizada en aplicaciones GIS para estimar la intensidad de una determinada variable en cualquier punto del área analizada, incluso en aquellos donde no hay observaciones. La idea es asumir que los valores observados corresponden a una distribución continua sobre el espacio, y determinar cual es la más probable en base a los puntos con datos. No hace falta realizar ningún cálculo matemático, sólo usar `geom_density2d` así:

```{r dpi=150}
ggmap(CABA) +
    geom_density2d(data = delitos, aes(x = longitud, y = latitud, color = stat(level))) +
    scale_color_viridis_c()
```

### Visualizando multiples categorías

Hasta aquí hemos analizado la distribución espacil del delito en su totalidad, sin diferenciar su tipo. Veamos ahora las difrencias por categoría. Podemos reintentar el mapa de puntos, esta vez filtrando los tipos de delito para incluir sólo los más frecuentes, y diferenciándolos por color.

```{r dpi=150}
ggmap(CABA) +
    geom_point(data = filter(delitos, tipo_delito %in% delitos_frecuentes), 
               aes(x = longitud, y = latitud, color = tipo_delito),
               size = 0.1, alpha = 0.1)
```

Aquí tenemos dos problemas: 

* La leyenda ("tipo_delito") es difícil de leer, dado que muestra los puntos tal como los definimos: pequeños y con mucha transparencia. Esos atributos son útiles en el mapa, donde tenemos cientos de miles de puntos, pero muy poco prácticos para la leyenda, donde sólo hay uno por etiqueta.

* Los puntos sobre el mapa se superponen en tal medida que es dificil discenir patrones espaciales distintos según su categoría.

El primer problema se resuelve fijando "a mano" los atributos de la lyenda, asi:

```{r dpi=150}
ggmap(CABA) +
    geom_point(data = filter(delitos, tipo_delito %in% delitos_frecuentes), 
               aes(x = longitud, y = latitud, color = tipo_delito),
               size = 0.1, alpha = 0.1) +
    guides(color = guide_legend(override.aes = list(size=2, alpha = 1))) +
    scale_color_brewer(palette = "Set1")
```

El segundo, usando facetado para mostrar en su propio mapa a cada categoría:

```{r dpi=150}
ggmap(CABA) +
    geom_point(data = filter(delitos, tipo_delito %in% delitos_frecuentes), 
               aes(x = longitud, y = latitud, color = tipo_delito),
               size = 0.1, alpha = 0.1) +
    scale_color_brewer(palette = "Set1") +
    facet_wrap(~tipo_delito)
```

El facetado ayuda. Se hacie evidente, por ejemplo, que el patrón espacial de las lesiones en seguridad vial es muy distinto al de hurto automotor.

Para hacer las diferencias aún mas nítidas, podemos facetar una estimación de densidad:

```{r dpi=150}
ggmap(CABA) +
    geom_density2d(data = filter(delitos, tipo_delito %in% delitos_frecuentes), aes(x = longitud, y = latitud, color = stat(level))) +
    scale_color_viridis_c() +
    facet_wrap(~tipo_delito)
```


### Combinando espacio y tiempo


El facetado también nos permite visualizar el cambio de posición a través del tiempo.

Por ejemplo, podemos comparar dos tipos de delito (hurto sin violencia y hurto de rueda) mostrando dónde ocuren en cada día de la semana.

```{r dpi=150}
delitos <- delitos %>% 
    mutate(dia_semana = wday(fecha, label = TRUE))

ggmap(CABA) +
    geom_point(data = filter(delitos, 
                             tipo_delito %in% c("Hurto (Sin violencia)", "Hurto De Rueda")),
               aes(x = longitud, y = latitud, color = tipo_delito), alpha = .5, size = .2) +
    facet_wrap(~dia_semana)

```

O concentrarnos en un tipo de delito en particular, y evaluar en que zonas se concentra de acuerdo a la hora del día:

```{r dpi=150}
delitos <- delitos %>% 
    mutate(hora_base = hour(hms(hora)))

ggmap(CABA) +
    geom_density2d(data = filter(delitos, 
                                 tipo_delito == "Hurto (Sin violencia)",
                                 !(wday(fecha) %in% 2:5) ),
                   aes(x = longitud, 
                       y = latitud, 
                       color = stat(level))) +
    scale_color_viridis_c() +
    facet_wrap(~hora_base, nrow = 4) +
    labs(title = "Concentración espacial de hurtos",
         subtitle = "según hora del día")
```


<!--chapter:end:05-analisis_espaciotemporal.Rmd-->

# Analizando movimiento: el flujo de viajes urbanos

Los sistemas urbanos se caracterizan por dinámicas continuas de flujo, como el viaje de las personas entre su lugar de trabajo y de residencia. Estas dinámicas son capturadas en diversas bases de datos con creciente grado de granularidad espacio-temporal. La disponibilidad de coordenadas precisas de origen y destino, combinada con la posibilidad de acceder a sistemas de ruteo en calles, nos permite estimar los trayectos realizado por personas y vehículos representados en bases de datos.

## Estimando rutas

En general, los datos de flujo disponibles en datasets a escala metropolitana (en contraste con los datos personales como los de GPS) son simples pares origen/destino. Una ejemplo de datos abiertos de este tipo, es el de la ubicación y intercambio entre estaciones de sistemas de bicicletas compartidas.

Por ejemplo, el portal de datos abiertos de la [Ciudad de Buenos Aires ofrece datasets con los trayectos realizados por los usuarios del sistema de bicicletas públicas, así como la ubicación de las estaciones](https://data.buenosaires.gob.ar/datasets?query=bicicletas).


Si no lo hemos hecho aún, carguemos las librerías que vamos a necesitar.

```{r}
library(tidyverse)
library(ggmap)
```

Utilizaremos una porción de todos los trayectos disponibles, los que representan viajes en bicicletas públicas realizados durante el mes de abril de 2017:


```{r}
viajes <- read_csv("https://bitsandbricks.github.io/data/viajes_BA_bici_abril_2017.csv")

viajes
```

También descargamos un archivo de información geográfica con la posición de cada estación de bicicletas públicas:

```{r}
estaciones <- read.csv("https://bitsandbricks.github.io/data/estaciones_BA_bici.csv")

estaciones
```

Ahora, las visualizamos. 

Como preparativo obtenemos una "bounding box", la caja de coordenadas que contiene todos los puntos:

```{r}
bbox <- make_bbox(estaciones$X, estaciones$Y)

bbox
```

Ahora descargamos un mapa que abarca el rectángulo de nuestra bounding box

```{r}

mapa_base <- get_stamenmap(bbox, color = "bw", zoom = 12)

ggmap(mapa_base) +
    geom_point(data = estaciones, aes(x = X, y = Y), color = "limegreen")

```

Podemos ver que las estaciones del sistema se concentran en el centro económico de la ciudad y sus zonas aledañas. No tenemos un campo con la fecha de inauguración que nos permita saber el orden en que se desplegaron las estaciones, pero podemos usar el número que les fue asignado (asumiendo que respetan un orden cronológico) para aproximarlo:

```{r}
ggmap(mapa_base) +
    geom_point(data = estaciones, aes(x = X, y = Y, color = NRO_EST)) +
    scale_color_distiller(type = "div")
```

Si el número de estación refleja la antigüedad, pareciera que primero de desplegó un corredor desde el _downtown_ hacia el noroeste, que luego se fue complementando con expansión radial.

## Cuantificando interacción 

A partir de ahora, agreguemos `theme_nothing()` para retirar todos los componentes auxiliares (como escalas y leyendas) y quedarnos solo con el mapa.

```{r}
ggmap(mapa_base) +
    geom_point(data = estaciones, aes(x = X, y = Y), color = "limegreen", size = 2) + 
    theme_nothing()
```


A continuación, realizamos un conteo de trayectos entre pares de estaciones

```{r}
conteo <- viajes %>% 
    group_by(ORIGEN_ESTACION, DESTINO_ESTACION) %>% 
    summarise(total = sum(TOTAL))
```


Podemos evaluar el grado de interconexión haciendo un _heatmap_, un mapa de calor que muestre la cantidad de viajes entre pares de estaciones. Hacemos uso de `geom_tile()` una geometría de `ggplot()` que genera rectángulos.

```{r}
ggplot() + 
    geom_tile(data = conteo, aes(x = ORIGEN_ESTACION, y = DESTINO_ESTACION, fill = total)) +
    scale_fill_distiller(palette = "Spectral")

```

El gráfico revela una característica de los datos: la numeración de la estaciones es discontinua. Crece secuencialmente hasta casi 200, pero por alguna hay un par de estaciones numeradas por encima de 500. Lo verificamos:

```{r}
unique(conteo$ORIGEN_ESTACION)
```

Podemos evitar el hueco que aparece en el mapa de calor tratando a las estaciones como una variable categórica (un factor) en lugar de numérica

```{r}
ggplot() + 
    geom_tile(data = conteo, 
              aes(x = as.factor(ORIGEN_ESTACION),
                  y = as.factor(DESTINO_ESTACION),
                  fill = total)) +
    scale_fill_distiller(palette = "Spectral")
```

La visualización es difícil de leer, pero aún así revela patrones. El tipo de viaje más popular es el de tomar y dejar la bicicleta en la misma estación, sugiriendo la prevalencia del uso recreativo. La interacción entre estaciones más alta se da entre las que tienen los primeros números, que como hemos visto se localizan en el centro de la ciudad. Las cantidad de combinaciones posibles crece rapidísimo con el número de nodos, por eso las interacción en redes grandes es difícil de visualizar.

Para continuar, tomemos sólo los 10 trayectos más frecuentes, descartando los viajes "circulares" (con el mismo origen y destino):

```{r}
top10 <- conteo %>% 
    ungroup() %>% 
    filter(ORIGEN_ESTACION != DESTINO_ESTACION) %>% 
    top_n(10)

top10
    
```

```{r}
ggplot() + 
    geom_tile(data = top10, 
              aes(x = as.factor(ORIGEN_ESTACION),
                  y = as.factor(DESTINO_ESTACION),
                  fill = total)) +
    scale_fill_distiller(palette = "Spectral")
```


Como se vislumbra en el heatmap completo, la interacción entre las estaciones 1 y 2 es con diferencia la más frecuente.

## Estimando rutas

Para trazar los trayectos de los usuarios al viajar de una estación a otra, no tendría sentido tender líneas rectas entre origen y destino. Para visualizar el tránsito, necesitamos tener en cuenta la ubicación de las calles y la dirección de tráfico que permiten. Lo ideal sería poder representar la ruta exacta de cada trayecto, sabiendo cuáles fueron las calles transitadas para realizar el viaje. Cuando no disponemos de información con ese nivel de detalle, lo que podemos hacer es estimar los recorridos utilizando un servicio de ruteo como el de Google Maps, o el del proyecto [OSRM](http://map.project-osrm.org/). 

En R contamos con paquetes especializados para conectar con estos servicios y trabajar con información de ruteo. El paquete `googleway()` permite conectar R con la API de Google Maps, y `osrm` hace lo propio con OSRM.

Vamos con OSRM. Si no tenemos el paquete necesario, lo instalamos.

```{r eval=FALSE}
install.packages("osrm")
```

Y lo activamos:

```{r}
library(osrm)
```

Para poder recibir información de ruteo desde los servidores de Google, la compañía exige el uso de una _API key_, una clave de autorización. Tal como con Twitter, el proceso de adquirir una clave es instantáneo, pero desde mediados de 2018 Google entrega API key sólo a usuarios que brinden información de una tarjeta de crédito, para cobrar el uso que supere ciertos umbrales. 

Para quienes deseen hacer uso de las múltiples funciones que Google ofrece a través de sus APIs, la molestia vale la pena, y puede seguir éstos pasos:  https://developers.google.com/maps/documentation/directions/get-api-key.

Para resolver el problema del ejercicio, nosotros optaremos por el ruteo vía OSRM que no requiere permiso ni tarjetas de crédito. 

Para encontrar una ruta, usamos la función `osrmRoute`, que requiere origen y destino en forma de vectores conteniendo un identificador (nombre del lugar o posición), longitud y latitud. Por ejemplo, para rutear entre dos lugares en Buenos Aires como Parque centenario y la estación Retiro:

```{r}
pcentenario <- c(nombre = "Parque Centenario",
                 lon = -58.435609,
                 lat = -34.606411)

eretiro <- c(nombre = "Estación Retiro",
             lon = -58.374873,
             lat = -34.591394)

centenario_a_retiro <- osrmRoute(src = pcentenario, 
                                 dst = eretiro, 
                                 returnclass =  "sf", 
                                 overview = "full")

```

La opción `returnclass = "sf"` permite obtener un dataframe espacial como resultado, que podemos proyectar luego sobre un mapa. `overview = "full"` hace que `osrmRoute` calcule la ruta precisa (con posiciones exactas) en lugar de un aproximado; de nuevo, solicitamos esto para luego poder visualizar el camino exacto en un mapa.

`osrmRoute` también estima la duración (en minutos) y la distancia (en kilómetros) del trayecto, como se ve en los campos "duration" y "distance":

```{r}
centenario_a_retiro
```

Podemos revisar rápidamente la ruta hallada usando `leaflet`:

```{r eval=FALSE}
library(leaflet)

leaflet(centenario_a_retiro) %>% 
    addTiles() %>% 
    addPolylines(color = "red")
```

```{r echo=FALSE}
library(leaflet)

leaflet(centenario_a_retiro) %>% 
    addProviderTiles(providers$OpenStreetMap) %>%
    addPolylines(color = "red")
```


Ahora, lo intentamos con los datos de viajes en bicicleta. Hacemos un _join_ del dataframe con el conteo de viajes contra el de posición de estaciones, para agregar las coordenadas.

De origen:

```{r}
top10 <- top10 %>% 
    left_join(estaciones[c("X", "Y", "NOMBRE", "NRO_EST")], 
              by = c("ORIGEN_ESTACION" = "NRO_EST")) %>% 
    rename(ORIGEN_X = X,
           ORIGEN_Y = Y,
           ORIGEN_NOMBRE = NOMBRE)

top10

```

Y además las de destino:

```{r}
top10 <- top10 %>% 
    left_join(estaciones[c("X", "Y", "NOMBRE", "NRO_EST")], 
              by = c("DESTINO_ESTACION" = "NRO_EST")) %>% 
    rename(DESTINO_X = X,
           DESTINO_Y = Y,
           DESTINO_NOMBRE = NOMBRE)

top10
```


Probemos rutear el trayecto más popular, el de Facultad de Derecho a Retiro:

```{r}
viaje <- top10[1,]

fderecho_a_retiro <- osrmRoute(src = c(viaje$ORIGEN_NOMBRE, viaje$ORIGEN_X, viaje$ORIGEN_Y), 
                                 dst = c(viaje$DESTINO_NOMBRE, viaje$DESTINO_X, viaje$DESTINO_Y), 
                                 returnclass = "sf", 
                                 overview = "full")

fderecho_a_retiro
```

```{r eval=FALSE}
leaflet(fderecho_a_retiro) %>% 
    addTiles() %>% 
    addPolylines(color = "red")
```

```{r echo=FALSE}
leaflet(fderecho_a_retiro) %>% 
    addProviderTiles(providers$OpenStreetMap) %>%
    addPolylines(color = "red")
```


Si queremos ver el trayecto en un mapa estático, podemos usar `ggmap()` con `geom_sf()`:

```{r}
ggmap(mapa_base) +
    geom_point(data = estaciones, aes(x = X, y = Y), color = "limegreen", size = 2) +
    geom_sf(data = fderecho_a_retiro, color = "red", inherit.aes = FALSE) +
    theme_nothing()

```

Calcular todos los recorridos y juntarlos en un sólo dataframe puede ser muy fácil o bastante engorroso, dependiendo de cuanta práctica tengamos en la automatización de tareas repetitivas. Por lo pronto, podemos descargar un dataset ya calculado con los recorridos detallados entre todas las estaciones de nuestro top 10:

```{r}
recorridos <- st_read("https://bitsandbricks.github.io/data/recorridos_BA_bici.geojson")

recorridos
```


Los que quieran espiar un método para compilar los recorrido por su cuenta, puede verlo al final del documento.


Para poder asignar un color a cada recorrido, creamos un identificador único para diferenciarlos

```{r}
recorridos <- recorridos %>% 
    mutate(ID = paste(ORIGEN_ESTACION, "-", DESTINO_ESTACION))
```

Y ahora, al mapa:

```{r}
ggmap(mapa_base) +
    geom_sf(data = recorridos, aes(color = ID), inherit.aes = FALSE) +
    theme_nothing()
```


Si queremos que el grosor de la línea represente la cantidad de veces que se realizó cada recorrido, primero agregamos la cantidad de viajes por recorrido, mediante el cruce con los datos que calculamos en "conteo": 


```{r}
recorridos <- recorridos %>% 
    left_join(conteo)
```

Y luego los usamos en el mapa:

```{r}
ggmap(mapa_base) +
    geom_sf(data = recorridos, aes(color = ID, size = total), alpha = 0.7, inherit.aes = FALSE) +
    theme_nothing()
```

También podemos usar el color para indicar el volumen de viajes:

```{r}
ggmap(mapa_base, darken = 0.7) +
    geom_sf(data = recorridos, aes(color = total, group = ID), 
              inherit.aes = FALSE,
              alpha = 0.7, size = 1.5) +
    scale_color_viridis_c(option = "inferno") +
    theme_nothing() 
```



## EXTRA: Cómo obtener las rutas de todos los recorridos

Tras leer el capítulo de 21 de R for Data Science, ["iteration"](http://r4ds.had.co.nz/iteration.html), ésto debería tener sentido:

```{r eval=FALSE}
obtener_recorrido <- function(o_nombre, o_x, o_y, d_nombre, d_x, d_y) {
    
    ruta <- osrmRoute(src = c(o_nombre, o_x, o_y),
                      dst = c(d_nombre, d_x, d_y),
                      returnclass = "sf")
    
    cbind(ORIGEN_ESTACION = o_nombre, DESTINO_ESTACION = d_nombre, ruta)
    
}


argumentos <- list(top10$ORIGEN_ESTACION, top10$ORIGEN_X, top10$ORIGEN_Y,
                  top10$DESTINO_ESTACION, top10$DESTINO_X, top10$DESTINO_Y)

recorridos <- pmap(argumentos, obtener_recorrido)

recorridos <- reduce(recorridos, rbind)
```

```{r ultmo_chunk_analisis_movimiento}
recorridos
```



<!--chapter:end:06-analisis_movimiento.Rmd-->

# Machine Learning (en una aplicación urbana)


El asíi llamado _machine learning_ consiste el empleo de aprendizaje estadístico automatizado para identificar patrones en grandes volúmenes de datos. El machine learning (de aquí en más ML) es utilizado en infinidad de campos debido a su creciente facilidad de uso y capacidad -en ciertos contextos- para predecir resultados con alta precisión.

Veremos como se aplica en la práctica con un ejercicio adaptado del [tutorial realizado por Cam Nugent](https://www.kaggle.com/camnugent/introduction-to-machine-learning-in-r-tutorial). Utiliza como fuente datos un [dataset](https://www.kaggle.com/camnugent/california-housing-prices/version/1) que contiene el valor mediano de las viviendas en California, EEUU, de acuerdo a un censo de 1990. ¡Es casi información arqueológica!

![](https://bitsandbricks.github.io/ciencia_de_datos_gente_sociable/imagenes/90210.jpg)

_Habitantes de California en los '90 con sus vestimentas nativas_


A pesar de no contener datos de factura reciente, el dataset ha aparecido en varios textos introductorios debido a que no contiene información sensible, es de dominio público, y sus variables son auto-explicativas. Además, su tamaño resulta adecuado: suficiente para no ser considerado "de juguete", pero no tan grande como para ser engorroso.

Cada fila representa un conjunto de manzanas, o "Census block groups". Por comodidad, nosotros les llamaremos distritos. El objetivo del ejercicio es predecir el valor medio de las viviendas de cada distrito en función de sus otros atributos conocidos: cuánta gente vive allí, donde esta localizado, si queda cerca del mar, la antigüedad de sus casas, etc. 

Allá vamos.

## Paso 0: Cargar paquetes

Además de las funciones de R "base", vamos a usar las del paquete `tidyverse` para procesar y visualizar nuestros datos, y las de `randomForest`, para aplicar el algoritmo de ML homónimo, que es relativamente simple y a la vez efectivo. 

```{r}
#install.packages("tidyverse")
library(tidyverse)
#install.packages("randomForest")
library(randomForest)
```


## Paso 1: Cargar los datos


Descargamos el dataset del siguiente modo:

```{r}
vivienda <- read_csv("http://bitsandbricks.github.io/data/housing.csv")
```


## Paso 2: Examinar los datos

Echamos un vistazo a las primeras filas,

```{r}
head(vivienda)
```

Y extraemos un resumen del contenido

```{r}
summary(vivienda)
```

Obsérvese que la variable "total_dormitorios" exhibe 207 datos faltantes, representados con `NA`. Luego volveremos a éste tema.

Y espiemos la distribución de algunas variables, como

ingresos:

```{r}
ggplot() +
    geom_histogram(data = vivienda, aes(x = mediana_ingresos))
```

antigüedad de las viviendas:

```{r}
ggplot() +
    geom_histogram(data = vivienda, aes(x = mediana_antiguedad_viviendas))
```

población:

```{r}
ggplot() +
    geom_histogram(data = vivienda, aes(x = poblacion))
```

... o proximidad al océano (dado que se trata de una variable categórica en lugar de continua, usamos un gráfico de barras en lugar de un histograma):

```{r}
ggplot() +
    geom_bar(data = vivienda, aes(x = proximidad_oceano))
```

## Paso 3: Limpiar los datos


### Imputar valores faltantes

Es habitual que los algoritmos empleados para ML no acepten datos faltantes. Es por eso que la limpieza básica de un dataset casi siempre incluye la imputación de datos no disponibles, evitando descartar por incompletas filas que contienen información valiosa en sus campos si disponibles.

Habíamos notado que la variable "total_dormitorios" tiene faltantes en 207 filas. Para esos casos, usaremos una de las técnicas de imputación mas _naif_ que hay: donde haya un valor desconocido, lo reemplazaremos por la mediana de los valores generales. 

```{r}
vivienda <- vivienda %>% 
    mutate(total_dormitorios = ifelse(is.na(total_dormitorios),
                                       median(total_dormitorios, na.rm = TRUE),
                                       total_dormitorios))
    
summary(vivienda)
```

Nos hemos librado de los `NA`.


### Normalizar variables

También deberíamos evaluar si algunas variables requieren ser normalizadas. En este caso las candidatas son "total_habitaciones" y "total_dormitorios", ya que es de esperarse que sean mayores donde hay más casas -naturalmente- pero eso no nos indica si las casas tienden a ser más o menos grandes que la media. Lo que haremos entonces es dividir la cantidad total de habitaciones por la cantidad de viviendas en cada distrito, y nos quedaremos con esas métricas en lugar de las originales:

```{r}
vivienda <- vivienda %>% 
    mutate(promedio_habitaciones = total_habitaciones/hogares,
           promedio_dormitorios = total_dormitorios/hogares) %>%
    select(-total_habitaciones, -total_dormitorios)

head(vivienda)
```

Del mismo modo sería mejor tener densidad de población en lugar de población total, pero sin saber el área que ocupa cada distrito no vamos a poder obtener la densidad. 


### Codificar variables categóricas

Ahora nos encargaremos de las variables categóricas, aquí representadas por "proximidad_oceano". Rara vez es posible utilizar columnas categóricas en modelos estadísticos, pero por suerte podemos recurrir a la alternativa de reemplazar una columna de datos categóricos por una serie de variables binarias, o "dummy". 

Es decir, en lugar de...


| caso | proximidad_oceano  |
|------|--------------------|
| A    | CERCANO A BAHIA    |
| B    | ALEJADO DEL OCEANO |
| C    | CERCANO A OCEANO   |


... tendríamos algo así como:


| caso | CERCANO A BAHIA | ALEJADO DEL OCEANO | ALEJADO DEL OCEANO |
|------|-----------------|--------------------|--------------------|
| A    | 1               | 0                  | 0                  |
| B    | 0               | 1                  | 0                  |
| C    | 0               | 0                  | 1                  |



Como buen lenguaje creado por y para practicantes del análisis estadístico, `R` trae una función específica para realizar ésta tarea, `model.matrix()`. Se usa así:


```{r}
matriz_categorias_oceano <- model.matrix(data = vivienda, ~ proximidad_oceano - 1)
```

y el resultado es, ni más ni menos, una matriz de variables binarias que representan las categorías originales:

```{r}
head(matriz_categorias_oceano)
```

Pronto agregaremos la matriz a nuestro dataframe, pero antes terminemos con otros ajustes pendientes.

### Unificar la escala de las variables numéricas

Éste paso siempre es necesario cuando estamos trabajando con variables que utilizan distintas unidades de medida. Aquí tenemos personas, hogares, habitaciones, dormitorios, años de antigüedad... de todo. Muchos algoritmos asumen que todas las variables tienen escalas comparables, lo cual genera problemas con las que alcanzan valores relativamente muy altos (como población, que llegar a decenas de miles) versus las que tienen rangos mucho menores (como antigüedad en años mediana, que "sólo" llega a 52). Si las dejásemos así, varias de las técnicas habituales del ML adjudicarían mucho más peso a las variables con números grandes, "despreciando" a las que por su naturaleza se mueven en rango más reducidos.

En todo caso, no importa lo disimiles que sean las unidades de medida, la solución es simple: convertimos todas las variables a la famosa "distribución Z", o función de estandarización, que convierte variables a una escala sin unidad de medida, que expresa cada valor como la cantidad de desvíos estándar que lo alejan de la media. Expresar todas las variables numéricas en forma de "z scores", o "valores z", las hace directamente comparables entre sí. 

En `R` disponemos de la función `scale()`, que obtiene los z-scores. Tomaremos entonces nuestro dataframe y usaremos `mutate_all()` para aplicar una función a todas las columnas restantes de un tirón. Eso si, quitando antes ciertas variables: las variables categóricas (que no tiene sentido pasar a z-scores porque no son variables numéricas), y la variable que estamos intentando predecir, ya que su escala no afecta los modelos y podemos dejarla en su formato original fácil de interpretar.

```{r}
vivienda <- vivienda %>% 
    select(-proximidad_oceano, -mediana_valor_vivienda) %>% 
    mutate_all(funs(scale)) %>% 
    mutate(mediana_valor_vivienda = vivienda$mediana_valor_vivienda)

```

Y obsérvese que `scale()` mediante, ahora todas las variables tienen promedio igual a 0, y se mueven en el mismo rango

```{r}
summary(vivienda)
```

... y sin que esto haya cambiado la forma de las distribuciones. Compárense ahora con las que examinamos al inicio, 

ingresos:

```{r}
ggplot() +
    geom_histogram(data = vivienda, aes(x = mediana_ingresos))
```

antigüedad de las viviendas:

```{r}
ggplot() +
    geom_histogram(data = vivienda, aes(x = mediana_antiguedad_viviendas))
```

población:

```{r}
ggplot() +
    geom_histogram(data = vivienda, aes(x = poblacion))
```

¡las formas son iguales! no hemos hemos perdido "información" respecto a que tan típico o extremo es cada valor, y hemos ganado la posibilidad de comparar en forma directa todas las variables: si un distrito tiene un valor cercano a cero en población, y  -digamos- más de 4 en ingresos, sabemos automáticamente que su población es parecida a la media de todos los distritos, pero sus ingresos son altísimos.


### Consolidar todas las variables generadas ad-hoc en un sólo dataframe 

Nos ha quedado por un lado un dataframe de variables numéricas estandarizadas, y por otro una matriz que representa la pertenencia de cada distrito a su categoría respecto a "proximidad al océano". 

Primero convertimos la matriz en dataframe (paso simple ya éstas estructura de datos son muy similares entre si), y luego unimos las columnas de ambos con la función `cbind()`:

```{r}
matriz_categorias_oceano <- as.data.frame(matriz_categorias_oceano)

vivienda <- vivienda %>% 
    cbind(matriz_categorias_oceano)
```

```{r}
head(vivienda)
```

Ya tenemos tenemos los datos limpios y en orden. 


## Paso 4: Crear sets de entrenamiento y de testeo

Para poder evaluar la calidad de un modelo predictivo, es práctica común dividir los datos disponibles en dos porciones. Una parte será utilizada para "entrenar" el modelo de ML, es decir se le permitirá al algoritmo acceder a esos datos para establecer la forma en que cada variable predictora incide en la que se quiere predecir. El resto será preservado y utilizado para "tomarle examen" al modelo: se le mostraran sólo las variables predictoras de esos datos, pidiendo al modelo una predicción del valor a estimar para cada una. Por último, contrastando aciertos y errores, se podrá establecer el grado de precisión del modelo. 

Incluso podríamos tener varios modelos distintos, obtenidos con distintas técnicas de ML. No es difícil, ya que una vez que los datos han sido obtenidos y preparados, nada impide usarlos como insumo de distintos algoritmos. En ese caso, se puede comparar la performance de los distintos modelos evaluando cual acierta mejor con la data de testeo.

Definamos entonces cuales filas van al set de entrenamiento, y cuáles al de testeo, eligiéndolas al azar. De acuerdo a distintas recetas, a veces se separa el 90% de los datos para entrenamiento y el resto para testeo, otras veces es mitad y mitad... ya que siempre es más o menos arbitrario, aquí usaremos el 80% para entrenar, y el 20% para testear.

```{r}
#definimos a mano la "semilla" de aleatorización para obtener resultados reproducibles
set.seed(1810)
```

Tomamos al azar el 80% de las posiciones entre 1 y la cantidad total de filas de nuestro dataset

```{r}
seleccion <- sample(1:nrow(vivienda), size = nrow(vivienda) * 0.8)

entrenamiento <- vivienda %>% 
    filter(row_number() %in% seleccion)

# el testeo es el set opuesto - aquellas filas cuya posición no está entre las seleccionadas
# el operador ! convierte una proposición en negativa 

testeo <- vivienda %>% 
    filter(!(row_number() %in% seleccion))

```

Ahora si, por fin, apliquemos un poco de machine learning.


### Paso 5: Entrenar y testear un modelo

Random Forest, una implementación de árboles de decisión como los ilustrados en ["Una introducción visual al machine learning"](http://www.r2d3.us/una-introduccion-visual-al-machine-learning-1/):

```{r}
modelo_RF <- randomForest(data = entrenamiento, mediana_valor_vivienda ~ .,
                         ntree = 500,
                         importance = TRUE)

# el parámetro "importance": Define si el modelo estimará la importancia relativa de cada predictor en la calidad de la predicción -es decir, cuales variables son más importantes para predecir

# resultados:
modelo_RF
```

Según dice allí, el modelo puede explicar más del 80% de la varianza de valores encontrada entre los distritos californianos en base a las variables predictoras que empleamos.


¿Qué tiene dentro el modelo?

```{r}
summary(modelo_RF)
```

De todo! Por ejemplo, "type" nos emite confirmar qué tipo de análisis realizó: Fue de regresión en este caso, peor podría haber sido otro, como clasificación (cuando se predice un atributo categórico en lugar de una variable continua):

```{r}
modelo_RF$type
```

O "importance", que contiene un ranking con la importancia relativa de cada predictor, es decir cuáles son los que más ayudan a estimar el valor a predecir:

```{r}
modelo_RF$importance
```

La columna "%IncMSE" representa el porcentaje de error promedio, la magnitud en la que el valor predicho por el modelo difiere del valor observado, cuando cada predictor se retira del modelo (es decir, cuanto peor sería la predicción si no se usara). Por eso los números mayores están asociados a los predictores de más peso, que en este caso son "mediana_ingresos", y luego longitud y latitud. Además de encontrar la correlación esperable entre nivel de ingresos de una población y el valor de sus viviendas, nuestro modelo ha encontrado que la ubicación es la clave del valor de la propiedad... y sin saber nada de geografía ni urbanismo.


En "predicted" tenemos la mediana del valor de la vivienda predicha para cada distrito:

```{r}
head(modelo_RF$predicted)
```


Aprovechando que dentro del modelo, "y" contiene los valores observados, evaluemos en forma gráfica cuánto se aproximan las predicciones de cada distrito al valor real (el observado) :

```{r}
ggplot() +
    geom_point(aes(x = modelo_RF$predicted, y = modelo_RF$y), alpha = 0.3) 
```

Se ajusta bastante bien. Luego veremos una manera de cuantificar la precisión del modelo. 


### Midiendo la performance del modelo contra datos que no conoce

Veamos ahora como se comporta nuestro modelo cuando debe predecir valores de distritos que no se han utilizado para el entrenamiento, los que reservamos para el set de testeo.

```{r}
predicciones_test <- predict(modelo_RF, newdata = testeo)

head(predicciones_test)
```

En un gráfico:

```{r}
ggplot() +
    geom_point(aes(x = predicciones_test, y = testeo$mediana_valor_vivienda), alpha = 0.3) 
```

Luce incluso mejor que el ajuste con los datos conocidos.

### Comparando performance

Es práctico obtener un sólo número, un indicador simple que nos diga que tan bien predice el modelo, y así poder comparar distintos modelos entre si (o distintos datasets contra el mismo modelo) utilizando esa medida. En estadística es común el uso del RMSE como indicador de grado de ajuste, o "Root Mean Square Error" - la raíz cuadrada de la media de los errores al cuadrado.

El modelo incluye el MSE (o sea la suma de los errores al cuadrado) que surge de comparar predicciones con valores observados. Y en el caso de un random forest, que intenta muchos árboles distintos, varios MSEs resultantes: 500 en nuestro caso, uno por cada árbol trazado. 

Tomamos la media de todos los MSE para obtener un valor general, y luego tomamos la raíz cuadrada para obtener el RMSE: 

```{r}
RMSE <- modelo_RF$mse %>% 
    mean() %>%
    sqrt()

RMSE
```

Eso significa que la diferencia promedio entre valor esperado y valor hallado para cada distrito fue de `r RMSE` dólares.

Y en comparación, ¿qué tan bueno resultó el modelo cuando se aplicó a datos que no conocía?


```{r}
RMSE_test <- sqrt(mean((predicciones_test - testeo$mediana_valor_vivienda)^2))

RMSE_test
```

Con un valor medio de error de `r RMSE_test` dólares, el modelo ha funcionado muy bien con datos desconocidos, incluso mejorando levemente su performance respecto al set de _training_. 

Esto indica que no sufre de "overfitting", la condición de estar excesivamente ajustado a los datos con los que fue entrenado. Por eso el modelo no pierde precisión cuando lidia con datos nuevos.

Como despedida, volvamos al examen visual. Representamos en un gráfico cada valor predicho y cada valor observado para los datos de entrenamiento:

```{r}
ggplot() +
    geom_point(aes(x = 1:length(predicciones_test), y = predicciones_test), 
               color = "salmon",
               alpha = .5,
               size = .5) +
    geom_point(aes(x = 1:nrow(testeo), y = testeo$mediana_valor_vivienda), 
               color = "lightblue",
               alpha = .5,
               size = .5) +
    labs(x = "valores predichos",
         y = "valores observados") +
    theme_minimal()
```

<!--chapter:end:07-machine_learning.Rmd-->

